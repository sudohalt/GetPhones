Paper 1

EE5364 Reading Assignment
Umayah Abdennabi (4210282) abden003@umn.edu November 2014

For our first paper we discuss The Future of Microprocessors, which was featured in Communications of the ACM, 2011[1].

Part I
This paper discusses the future trends and designs of microprocessors, and the new difficulties that will be faced when designing new microprocessors that are more efficient and perform better. The biggest factor influencing the future of microprocessor design is energy efficiency, this is the key limiter in performance. Microprocessors are one of the most important inventions of the 20th century that have changed the world. Therefore their future designs will have a very big impact on many industries and fields of study. The paper begins by discussing the design and advances of microprocessors for the past 20 years, and then moves on to discuss the current problems faced and what the future of microprocessor will likely be. Since there may be radical changes in microprocessosr designs the paper also discusses how this will change the way software is developed and the new tools that will be created for software development.
We learn in the first part of the paper that the past 20 years has seen significant advances in microprocessors due to transistor-speed scaling, microarchitectural advances, and cache memories. Transistor-speed scaling is when the size of the transistor is decreased, which then increases the density of transistors on a chip. Since the size is decreased there is a reduction in delay, which increases the speed. Industries have used Moore’s Law as a guideline for development. We see that for the past 20 years that to keep up with Moore’s Law industries have been scaling the dimension of the transistor by 30%, which reduces their area by 50% (doubling the transistor density on chip). This reduction in size increases performance because of a 0.7x delay reduction. To keep power consumption down voltage supply is reduced by 30% which reduces power consumption by 50%. This means a 40% faster processor, and a processor using the same amount of power (since we are adding more transistors now that they are smaller and take less power). This amazing engineering has been successful for over the past 20 years, but it has not been simple or easy to accomplish and every generation it becomes harder and harder. The next technique to use is changing and improving the microarchitecture. Superscalar designs, Pipelining, branch prediction, out of order execution, speculation, and plethora of other techniques are all part of microarchitecture. Some techniques have much better performance to energy consumption ratios compared to others. For example the authors show that deep pipelining has the lowest performance benefits compared to other techniques such as out or order or speculation which have the best performance. It is becoming increasingly difficult to implement a new microarchitecture every generation and the gains have been less then other techniques such as transistor scaling, and caches which will be discussed next.
1
The third technique used is cache memory architecture. Originally microprocessors did not contain cache’s (on chip memory) and used DRAM for memory. Engineers noticed this bottleneck and introduced caches into the design of microprocessors. The introduction of caches allowed DRAM’s to increase in memory density and lower in cost, while not necessarily increasing in speed (the speed of DRAM is orders of magnitude slower than the CPU cyle), but this is mitigated by caches which give the illusion of a system as large as the available memory but almost as fast as the CPU cycle. With the increasing demand for more memory and better performance multiple levels of cache were introduced, so we now see processors with level 2 and level 3 caches (each level having larger memories but slower speeds). Caches are now being used to improve performance when possible instead of microarchitecture, because they require much less power and are much more energy efficient. So we are now seeing more and more die area being dedicated to caches (and there are optimal points to reach between area dedicated to cores, and those dedicated to caches).
We saw a microprocessors improve 1000 fold the past 20 years, and in the second half of the paper the authors discuss how the method previously used are alone no longer feasible. The size of transistors are now reaching the size where physical phenomena are playing a more and more important role. The size of transistors have reached atomic levels, and it is no longer feasible to continue in the same fashion that we have been for the past 20 years. The authors mention how the threshold voltage is now reaching a point where if it is decreased small amounts of current are leaked , and this leakage increases exponentially with threshold reduction. Since the transistor is not a perfect switch this is becoming a serious problem. Therefore a large amount of power consump- tion is due to leakage, and to fix this threshold voltage can no longer be decreased, which reduces transistor performance. As mentioned previously transistors are reaching sizes where lithography is becoming extremely difficult. Therefore the power and performance benefits of scaling the tran- sistor has become less. Increasing the amount of transistor and speed is hindered by the amount of power it would consume. There is balance to be had with cache size and logic design, to meet the power requirements. The authors show how power consumption has become the leading hindrance in microprocessor design, and chip designers need to rethink processor architecture and implemen- tation. Single threaded processors are no longer feasible and multi threaded performance is now necessary. The authors discuss multicore designs and customizations. Multicore designs allow for threads to execute in parallel, exploiting Moore’s Law through core replication. There are multiple choices to be made in how many cores and the size of the cores. One can have a few large cores, or many small cores, or a hybrid of both. Hardware customization include fixed-function accelerators, FGPA’s, and programmable accelerators. Accelerators are dedicated hardware to perform some function, instead of having to perform it in software[3]. Examples include media, cryptographic, and composting accelerators. These accelerators are capable of 500x performance improvement for certain applications, as compared with a general purpose CPU. Also GPU fall into the category of customizations. The authors show that future trends will have heterogeneous designs with CPUS, accelerators, GPUs, and others. Where the CPU will be orchestrating between all the different components. The authors talk about how the movement of data and wire connects impacts en- ergy consumption severely if done poorly. Therefore having local data and memory is becoming increasingly important. A badly connected chip can have data travelling longer distances then it has to, or having to go through inefficient paths greatly increasing energy consumption. The last thing the authors discuss before moving on to the conclusion is the challenges faced in software. Some of the trends of microprocessors, such as memory hierarchy, customized hardware, and added accelerators greatly influence the software that will be written for theses systems. The authors state that some of the biggest changes will be the abandonment of hardware support for a single address space, a single flat address space (and instead a more hierarchical space), and steady rates of execution. Also software engineers and programmers will have start thinking in parallel and
2
multi threaded algorithms instead of single threaded algorithms. The changes in microprocessors will mostly impact the designers of operating systems, compilers, and those designing the libraries and API’s necessary to interact with the system and its accelerators and customized hardware. The paper concludes with an overview of the whole paper, and also has a paragraph talking about innovative alternate technologies such as quantum electronics, graphene, and carbon nanotubes. In the end the authors state that the future winners of microprocessors is not clear, and that the new technologies adds a whole new exciting dimension to look into.
Part II
The paper was a discussion about microprocessors and their history and future. Therefore when assessing the main results of this type of paper we have to look into whether they successfully conveyed the information they were presenting. The main results I believe what the authors were tying to show was how the next couple decades of processor design will be more challenging and exciting, and then many techniques that were previously used will no longer be as effective as they used to. The authors showed this by discussing that past 20 years of processor improvements, and the techniques that chip designers used. They then discussed the problems facing chip designers today, mainly energy consumption, and how previous techniques will no longer be as effective as they used to. They then present different trends a techniques that are currently being used, and will most likely be used in the future. I believe they successfully presented their results, and conclusion. Their results being the history and future of microprocessor designs, and the conclusion being and overview of this and what is in store for the future.
Part III
The paper discusses something which impacts nearly every modern industry and field. The authors gave a good overview of the problems that are facing microprocessors, and what the future looks like. They also gave an excellent overview of the previous 20 years of microprocessors. There were many diagrams that gave a good visual description of the topics they were describing that solidified my understanding. The authors should have discussed more about why energy consumption is a bad thing in terms of microprocessors. Since they state that it is a key limiter in performance, they should have spent a good portion describing why that is. In the paper I did not see this, and the authors just talked about how certain designs are more energy efficient then others or that a certain design consumes too much energy, but they did not talk about why this is a problem. Another thing the paper did not talk enough about was alternate transistor designs. In the conclusion the authors briefly mention carbon nanotubes, graphene, and quantum electronics. The focus of the paper was on silicon based microprocessors, but the authors should have set aside a section discussing alternate materials. This is because these alternate designs might t play a large role in microprocessors, and may replace the conventional silicon chip. Another thing that would have been nice for the paper to discuss would be other processor companies. Intel is the biggest microprocessor company, and their capabilities and resources dwarf nearly every other companies. The problems Intel is facing is not necessarily the problems other companies way behind Intel are facing. So giving a general overview of the current microprocessor market would have been nice. Overall the paper was very well written and gave an excellent overview of the past 2 decades of microprocessors, and what is in store for them in the next coming decades. One thing that I found very interesting about the future of microprocessors is the way that it will start to impact programmers and software engineers. It seems as though that the future for them will be similar to programmers in the early years of computers where they had to be very conscious about the amount of resources their
3
programs used.
Paper 2
For our second paper we discuss When Prefetching Works, When It Doesn’t, and Why, which was featured in ACM Transactions on Architecture and Code Optimization, 2012[4]
Part I
This paper discussed the benefits and limitations of software and hardware prefetching. Real world software and hardware prefetching mechanisms (and combinations of the two) are analyzed on different benchmarks. The authors begin with an introduction which addresses the existing body of work and how the work the authors are providing is new, and adds to the body. The com- plexity of the interaction between software and hardware prefetching, and the reasons for software or hardware prefetching performance was not well understood. Also there were few guidelines to when to insert prefetch intrinsics in code (for software prefetching, or training the hardware prefetcher). The authors want to address these concerns with their work. The authors clearly stated that their goals were to provide guidelines for inserting prefetching intrinsics in the presence of hardware prefetcher, and also to identify promising new directions for automated cooperative software/hardware prefetching schemes. Another thing that the authors were looking to provide in this work was explanations for why certain benchmarks (such as software and hardware inter- actions) acted in the way that they did whether it be positive, neutral, or negative. Overall the authors are attempting to give more insight into hardware and software prefetching.
The author goes through many tests and simulations throughout the paper further supporting the authors results and conclusions. We see how software prefetching performs great in many scenarios that hardware is unable to. Many benchmarks are used as proof to their results. This paper used many graphs and simulations to validate their conclusions, which made them extremely powerful. This is important since there are instances where hardware prefetchers are able to perform better than software, because they have the added dynamic capability of changing during runtime. Having these two together can provide great prefetching, but it is also important to know when and how (if they do interact). If the software prefetcher interacts with the hardware prefetcher, by maybe training it, results can be good, but in many cases catastrophic.
We see many cases in which where prefetch intrinsics are placed greatly affects the performance of the prefetching, and that poorly placing these instrinsics greatly reduces the effectivness of the software prefetching.
Part II
There was a lot of work that was put into this paper, and a vast amount of data and graphs showing their results. From their results they showed the scenarios in which software was better than hardware, and vice versa. They were also able to show how they interacted with each other. Their conclusion showed that software prefetching was best at large numbers of streams because of the potential hardware limit, short streams since it is difficult for hardware to learn with short streams, irregular memory access, l1 data placement in cache compared to hardware which typically places in lower level caches (this is also difficult to change since it is in hardware as compared to software), and determining loop bounds. However software prefetchers increased the instruction count sometimes by over a 100%. The static nature of software prefetchers. Unlike software hardware can change dynamically to any changes in run time and adpapt, whereas software prefetchers do all their
4
optimizations before the code runs. The authors were also able to show how software prefetching can help teach hardware prefetchers, but can degrade performance greatly and it is best to exercise caution when teaching hardware from software prefetcher. The authors gave a good overview of their benchmarks and why certain benchmarks performed moor poorly. For example some benchmarks performed poorly with hardware benchmarks, and they were able to describe why this happened by showing how this particular benchmark was using very short streams. Also the authors provided a nice code examples of where prefetch instructions should be placed, and how it is necessary many times to insert your own prefetch statements since compilers tend to be very judicial in when they place these instructions.
Part III
This paper was very solid in that it conveyed all its findings with hard numbers and graphs, and gave explanations for all the findings. All the experiments that were run were explained, and a reason given for their purpose. The paper was very scientific, and much analysis and conclusions can be made from the facts shown. However there were some aspects of the paper that made difficult to understand the place or hardwar and software prefetching. The paper seemed to talk much more about software prefetching than hardware prefetching. This made it very difficult to conclude the place of hardware prefetching in a computers architecture. Another thing that would have been nice would have been for them to describe how software or hardware prefetch algorithms worked, and relate their description to their results. This way it would be easy to see what strucutres or algorithms were causing what.
References
1. The Future of Microprocessors,
May 2011, vol. 54, no. 5, Communications of the ACM
2. http://en.wikipedia.org/wiki/Intel Tick-Tock
3. http://en.wikipedia.org/wiki/Hardware acceleration
4. When Prefetching Works, When It Doesn’t, and Why,
March 2012, Vol. 9, No. 1, Article 2, ACM Transactions on Architecture and Code Opti- mization
￼￼5
`
